# DM2024-Lab2-master
ISA5810 Data Mining Lab 2

<<<<<<< HEAD
本次作業共使用了七種分析方法，以下是作業紀錄。
=======
本次Kaggle競賽共使用六種分析方法，以下是作業紀錄。
>>>>>>> ba8d73df108eff2011f77dedc2114a74f49143cb

| **方法**                  | **特徵工程**                | **模型**                    | **準確率預估** | **優勢**                                     | **劣勢**                                      | **GPU 支援**            |
|--------------------------|----------------------------|-----------------------------|----------------|---------------------------------------------|----------------------------------------------|-------------------------|
| **TF-IDF + 隨機森林**      | 稀疏特徵表示，詞頻與逆文檔頻率權重 | 隨機森林                    | 75%-82%       | 模型穩定性強，對噪聲和高維數據不敏感          | 無法處理非線性模式，對語義信息利用不足           | 不支持                  |
| **TF-IDF + Boosting**      | 稀疏特徵表示，詞頻與逆文檔頻率權重 | XGBoost 或 LightGBM         | 78%-85%       | 擅長處理稀疏特徵，對錯分樣本有良好適應能力      | 訓練成本略高，需調參以達到最佳效果              | 支持（顯著加速，適合大數據集）|
| **Word2Vec + 隨機森林**     | 詞嵌入，計算句向量平均值       | 隨機森林                    | 72%-80%       | 能結合詞嵌入語義特徵，提升語義捕捉能力          | 詞嵌入需預處理，隨機森林對非線性語義的處理有限     | 不支持                  |
| **Word2Vec + CNN**         | 詞嵌入，保留語序            | 卷積神經網絡                | 75%-85%       | 捕捉局部語義特徵，對短文本效果佳               | 訓練需較多資源，對長文本效果有限               | 支持（顯著加速）         |
| **BERT 嵌入 + Transformer**| 上下文語義嵌入，保留全局語義 | 預訓練 BERT 模型             | 85%-90%       | 能捕捉上下文語義，分類準確率最高               | 訓練和推理成本高，需要大量數據和資源支持         | 支持（必要，否則速度較慢） |
| **Tokenizer + LSTM**       | 數字化文本序列，保留序列上下文 | 長短期記憶神經網絡           | 80%-88%       | 能捕捉文本序列特徵，適合時間序列或長文本        | 訓練成本中等，對長文本可能有梯度消失問題         | 支持（顯著加速，必要）   |
| **RoBERTa + Transformer**       | 上下文語義嵌入，為下游任務進行最佳化 | 預訓練的RoBERTa模型           | 88%-92%       | 增強的訓練優化，更擅長處理長序列        | 類似於BERT，但具有較高的計算成本和資源需求         | 適用於（且對大型任務是必要的） |